# 7天用Go从零实现分布式缓存GeeCache

# 自学补充知识
Day1 
    reflect.DeepEqual

#Day0 序言
1、分布式缓存
第一次请求时将一些耗时操作的结果暂存，以后遇到相同的请求，直接返回暂存的数据。我想这是大部分童鞋对于缓存的理解。
在计算机系统中，缓存无处不在，
比如我们访问一个网页，网页和引用的 JS/CSS 等静态文件，根据不同的策略，会缓存在浏览器本地或是 CDN 服务器，那在第二次访问的时候，就会觉得网页加载的速度快了不少；
比如微博的点赞的数量，不可能每个人每次访问，都从数据库中查找所有点赞的记录再统计，数据库的操作是很耗时的，很难支持那么大的流量，所以一般点赞这类数据是缓存在 Redis 服务集群中的。

商业世界里，现金为王；架构世界里，缓存为王。

内存不够：合理的淘汰策略
并发写入：锁
单机性能：水平扩展多机支持分布式，垂直扩展增加单个节点的计算、存储、带宽。（但大部分情况下，分布式系统是一个更优的选择）


2 关于 GeeCache
设计一个分布式缓存系统，需要考虑资源控制、淘汰策略、并发、分布式节点通信等各个方面的问题。而且，针对不同的应用场景，还需要在不同的特性之间权衡，例如，是否需要支持缓存更新？还是假定缓存在淘汰之前是不允许改变的。不同的权衡对应着不同的实现。

groupcache 是 Go 语言版的 memcached，目的是在某些特定场合替代 memcached。groupcache 的作者也是 memcached 的作者。无论是了解单机缓存还是分布式缓存，深入学习这个库的实现都是非常有意义的。

GeeCache 基本上模仿了 groupcache 的实现，为了将代码量限制在 500 行左右（groupcache 约 3000 行），裁剪了部分功能。但总体实现上，还是与 groupcache 非常接近的。支持特性有：

单机缓存和基于 HTTP 的分布式缓存
最近最少访问(Least Recently Used, LRU) 缓存策略
使用 Go 锁机制防止缓存击穿
使用一致性哈希选择节点，实现负载均衡
使用 protobuf 优化节点间二进制通信

#Day1 LRU缓存淘汰策略
介绍常用的三种缓存淘汰(失效)算法：FIFO，LFU 和 LRU
实现 LRU 缓存淘汰算法，代码约80行

#Day2 单机并发缓存
介绍 sync.Mutex 互斥锁的使用，并实现 LRU 缓存的并发控制。
实现 GeeCache 核心数据结构 Group，缓存不存在时，调用回调函数获取源数据，代码约150行

#Day3 HTTP服务端
介绍如何使用 Go 语言标准库 http 搭建 HTTP Server
并实现 main 函数启动 HTTP Server 测试 API，代码约60行

#Day4 一致性哈希hash
一致性哈希(consistent hashing)的原理以及为什么要使用一致性哈希。
实现一致性哈希代码，添加相应的测试用例，代码约60行